{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data Collection from CanLII API and Web Scraping of Case Decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Adjust Display Options for Pandas DataFrame\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Fetch API Key and Define Initial Parameters\n",
    "api_key = os.getenv('canl2_api_key')\n",
    "if not api_key:\n",
    "    raise ValueError(\"API key not found. Please set the 'canl_api_key' environment variable.\")\n",
    "\n",
    "# Define parameters for API call\n",
    "language = 'en'\n",
    "database_id = 'onhrt'  # Database ID for Ontario Human Rights Tribunal\n",
    "start_date = '2021-01-01'\n",
    "end_date = '2024-12-31'\n",
    "offset = 0\n",
    "result_count = 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Function to Fetch Case Decisions from the API\n",
    "def fetch_case_decisions(database_id, start_date, end_date, offset, result_count):\n",
    "    \"\"\"\n",
    "    Fetch case decisions from CanLII API within the specified date range.\n",
    "\n",
    "    Parameters:\n",
    "    - database_id: ID of the legal database (e.g., 'onhrt' for Ontario Human Rights Tribunal)\n",
    "    - start_date: Start date for case collection\n",
    "    - end_date: End date for case collection\n",
    "    - offset: Starting point for results\n",
    "    - result_count: Number of results to fetch per request\n",
    "\n",
    "    Returns:\n",
    "    - List of cases\n",
    "    \"\"\"\n",
    "    cases = []\n",
    "    while True:\n",
    "        url = (f'https://api.canlii.org/v1/caseBrowse/{language}/{database_id}/'\n",
    "               f'?offset={offset}&resultCount={result_count}&decisionDateAfter={start_date}'\n",
    "               f'&decisionDateBefore={end_date}&api_key={api_key}')\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        if 'cases' in data:\n",
    "            cases.extend(data['cases'])\n",
    "            if len(data['cases']) < result_count:\n",
    "                break\n",
    "            offset += result_count\n",
    "        else:\n",
    "            break\n",
    "    return cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Fetching Case Decisions and Creating DataFrame\n",
    "cases = fetch_case_decisions(database_id, start_date, end_date, offset, result_count)\n",
    "df = pd.DataFrame(cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 Data Cleaning: Extract 'caseId' and Remove '(CanLII)' from Citations\n",
    "df['caseId'] = df['caseId'].apply(lambda x: x.get('en') if isinstance(x, dict) else x)\n",
    "\n",
    "def remove_canlii(citation):\n",
    "    \"\"\"\n",
    "    Remove the '(CanLII)' text from citations.\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s*\\(CanLII\\)', '', citation)\n",
    "\n",
    "df['citation'] = df['citation'].apply(remove_canlii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.6 Checking for Null and NaN Values\n",
    "columns_to_check = ['databaseId', 'caseId', 'title', 'citation']\n",
    "null_counts = df[columns_to_check].isnull().sum()\n",
    "nan_counts = df[columns_to_check].applymap(lambda x: pd.isna(x) and not pd.isnull(x)).sum()\n",
    "\n",
    "print(\"Null values count per column:\\n\", null_counts)\n",
    "print(\"\\nNaN values count per column:\\n\", nan_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.7 Handling Missing Case IDs\n",
    "null_caseId_rows = df[df['caseId'].isnull()]\n",
    "nan_caseId_rows = df[df['caseId'].isnull()]\n",
    "\n",
    "def create_caseId(citation):\n",
    "    \"\"\"\n",
    "    Create case ID from the citation if caseId is missing.\n",
    "    \"\"\"\n",
    "    return citation.lower().replace(' ', '')\n",
    "\n",
    "df.loc[df['caseId'].isnull(), 'caseId'] = nan_caseId_rows['citation'].apply(create_caseId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.8 API Calls for Case Details\n",
    "# Reading caseId from existing file\n",
    "df = pd.read_csv(r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\case_ids.csv')\n",
    "caseId_list = df['caseId'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.9 Function to Retrieve Case Details from API\n",
    "def get_case_details(caseId):\n",
    "    url = f\"https://api.canlii.org/v1/caseBrowse/{language}/{database_Id}/{caseId}/?api_key={api_key}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        print(\"Http Error:\", errh)\n",
    "    except requests.exceptions.ConnectionError as errc:\n",
    "        print(\"Error Connecting:\", errc)\n",
    "    except requests.exceptions.Timeout as errt:\n",
    "        print(\"Timeout Error:\", errt)\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(\"Oops: Something Else\", err)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.10 Making API Calls and Saving Results\n",
    "cases_details = []\n",
    "caseId_list_5000 = caseId_list[:5000]\n",
    "caseId_list_383 = caseId_list[5000:]\n",
    "\n",
    "def make_api_calls(case_ids):\n",
    "    \"\"\"\n",
    "    Make API calls to retrieve case details for the provided list of caseIds.\n",
    "    \"\"\"\n",
    "    for caseId in case_ids:\n",
    "        case_details = get_case_details(caseId)\n",
    "        if case_details is not None:\n",
    "            cases_details.append(case_details)\n",
    "        time.sleep(1)  # Adjust the sleep time as per API rate limit policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 5000 API calls\n",
    "print(\"Making first 5000 API calls...\")\n",
    "make_api_calls(caseId_list_5000)\n",
    "\n",
    "# Saving results of the first 5000\n",
    "cases_df_5000 = pd.DataFrame(cases_details)\n",
    "cases_df_5000.to_csv(r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\metadata_50000.csv', index=False)\n",
    "print(\"First 5000 API calls completed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final 383 API calls\n",
    "print(\"Making final 383 API calls...\")\n",
    "make_api_calls(caseId_list_383)\n",
    "cases_df_383 = pd.DataFrame(cases_details)\n",
    "cases_df_383.to_csv(r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\metadata_383.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.11 Combining DataFrames and Final Output\n",
    "cases_df_5000 = pd.read_csv(r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\metadata_5000.csv')\n",
    "cases_df_383 = pd.read_csv(r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\metadata_383.csv')\n",
    "cases_df_combined = pd.concat([cases_df_5000, cases_df_383])\n",
    "cases_df_combined.to_csv(r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\metadata.csv', index=False)\n",
    "print(\"All API calls completed and combined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.12 Web Scraping Case Content\n",
    "# Function to fetch case content from URL\n",
    "def fetch_case_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        original_document_div = soup.find('div', {'id': 'originalDocument', 'class': 'solexHlZone lbh-document'})\n",
    "        if not original_document_div:\n",
    "            return \"Main content not found\"\n",
    "        return original_document_div.get_text(separator='\\n')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        if response.status_code == 429:\n",
    "            return \"Rate limit hit\"\n",
    "        print(f\"Failed to fetch {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.13 Saving Case Content\n",
    "def save_case_content(case_number, content):\n",
    "    with open(f\"cases/2024hrto{case_number}.txt\", 'w', encoding='utf-8') as file:\n",
    "        file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.14 Generating Case URLs and Scraping\n",
    "def generate_case_urls(start_case=1, end_case=966):\n",
    "    base_url = \"https://www.canlii.org/en/on/onhrt/doc/2022/\"\n",
    "    return [f\"{base_url}2022hrto{case_number}/2022hrto{case_number}.html\" for case_number in range(start_case, end_case + 1)]\n",
    "\n",
    "def scrape_cases(urls):\n",
    "    if not os.path.exists('cases'):\n",
    "        os.makedirs('cases')\n",
    "    last_successful_case = None\n",
    "    for url in urls:\n",
    "        case_number = url.split('/')[-1].replace('2022hrto', '').replace('.html', '')\n",
    "        print(f\"Fetching case {case_number}: {url}\")\n",
    "        case_content = fetch_case_content(url)\n",
    "        if case_content == \"Rate limit hit\":\n",
    "            print(f\"Rate limit hit at case {case_number}. Stopping...\")\n",
    "            break\n",
    "        elif case_content:\n",
    "            save_case_content(case_number, case_content)\n",
    "            print(f\"Saved case {case_number}\")\n",
    "            last_successful_case = case_number\n",
    "        else:\n",
    "            print(f\"Failed to save case {case_number}\")\n",
    "    if last_successful_case:\n",
    "        with open('last_successful_case.txt', 'w') as file:\n",
    "            file.write(last_successful_case)\n",
    "\n",
    "# Starting scraping process\n",
    "start_case = 1\n",
    "if os.path.exists('last_successful_case.txt'):\n",
    "    with open('last_successful_case.txt', 'r') as file:\n",
    "        start_case = int(file.read()) + 1\n",
    "\n",
    "urls = generate_case_urls(start_case=start_case)\n",
    "print(urls\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
