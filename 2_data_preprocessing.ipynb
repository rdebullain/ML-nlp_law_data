{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data Preprocessing for CanLII Case Decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import chardet\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Download NLTK Resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Adjust Display Options for DataFrame\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 Define Folder Path for Case Files\n",
    "folder_path = r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\cases'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 Initialize Empty List to Store Case Details\n",
    "case_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.6 Function to Detect File Encoding\n",
    "def detect_encoding(file_path):\n",
    "    \"\"\"\n",
    "    Detect the encoding of a file using chardet.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "    result = chardet.detect(raw_data)\n",
    "    return result['encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.7 Iterate Over Files in the Folder and Load Case Content\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        encoding = detect_encoding(file_path)\n",
    "        with open(file_path, 'r', encoding=encoding) as file:\n",
    "            content = file.read()\n",
    "            case_id = os.path.splitext(filename)[0]  # Remove .txt extension\n",
    "            case_list.append({'caseId': case_id, 'caseContent': str(content)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.8 Convert List to DataFrame\n",
    "cases = pd.DataFrame(case_list)\n",
    "\n",
    "# Ensure 'caseContent' column is of string type\n",
    "cases['caseContent'] = cases['caseContent'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.9 Display Basic Information About DataFrame\n",
    "cases.info()\n",
    "\n",
    "# Display the first case\n",
    "cases.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.10 Save Initial Case DataFrame to CSV\n",
    "output_path = r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\data\\cases.csv'\n",
    "cases.to_csv(output_path, index=False, encoding='utf-8')\n",
    "\n",
    "# Reload the saved DataFrame for further processing\n",
    "cases = pd.read_csv(r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\data\\cases.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.11 Function to Clean Text Content\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean the text by removing excessive newlines, whitespace, and underscores.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'_', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Apply the text cleaning function to 'caseContent' column\n",
    "cases['cleanSummary'] = cases['caseContent'].apply(clean_text)\n",
    "\n",
    "# Drop the original 'caseContent' column\n",
    "cases.drop(columns=['caseContent'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.12 Save Cleaned DataFrame to CSV (Optional)\n",
    "output_path = r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\data\\cleanSummary.csv'\n",
    "cases.to_csv(output_path, index=False, encoding='utf-8')\n",
    "\n",
    "# Reload the cleaned DataFrame for further processing\n",
    "cases = pd.read_csv(r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\data\\cleanSummary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case Details and Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.13 Function to Separate Case Details and Reasoning\n",
    "def separate_case_details(text):\n",
    "    \"\"\"\n",
    "    Split the text into case details and reasoning using a specific pattern.\n",
    "    \"\"\"\n",
    "    pattern = r'\\[\\s*1\\s*\\]'\n",
    "    parts = re.split(pattern, text, maxsplit=1)\n",
    "    case_details = parts[0].strip()\n",
    "    cleaned_summary = '[1]' + parts[1].strip() if len(parts) > 1 else ''\n",
    "    return case_details, cleaned_summary\n",
    "\n",
    "# Apply the function to the 'cleanSummary' column\n",
    "cases['caseDetails'], cases['reasoning'] = zip(*cases['cleanSummary'].apply(separate_case_details))\n",
    "\n",
    "# Drop the 'cleanSummary' column\n",
    "cases.drop(columns=['cleanSummary'], inplace=True)\n",
    "\n",
    "# Display basic info after separation\n",
    "cases.info()\n",
    "\n",
    "# Display first row\n",
    "cases.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.14 Save Separate Case Details and Reasoning to CSV\n",
    "output_path = r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\data\\reasoning.csv'\n",
    "cases[['caseId', 'reasoning']].to_csv(output_path, index=False, encoding='utf-8')\n",
    "\n",
    "output_path = r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\data\\caseDetails.csv'\n",
    "cases[['caseId', 'caseDetails']].to_csv(output_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applicant Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.15 Extract Applicant Names from Case Details\n",
    "def extract_applicant(text):\n",
    "    \"\"\"\n",
    "    Extract the applicant's name from case details using regex.\n",
    "    \"\"\"\n",
    "    between_pattern = r'(B\\s*E\\s*T\\s*W\\s*E\\s*E\\s*N\\s*[:\\s]*)'\n",
    "    applicant_pattern = r'(Applicant|Applicants)'\n",
    "    \n",
    "    between_match = re.search(between_pattern, text)\n",
    "    if between_match:\n",
    "        start = between_match.end()\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "    applicant_match = re.search(applicant_pattern, text[start:])\n",
    "    if applicant_match:\n",
    "        end = start + applicant_match.start()\n",
    "    else:\n",
    "        end = len(text)\n",
    "    \n",
    "    return text[start:end].strip()\n",
    "\n",
    "# Apply the function to extract applicants\n",
    "cases['applicant'] = cases['caseDetails'].apply(extract_applicant)\n",
    "\n",
    "# Reorder columns to have 'applicant' before 'caseDetails'\n",
    "cases = cases[['caseId', 'applicant', 'caseDetails']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.16 Save Applicant Data to CSV\n",
    "output_path = r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\data\\applicant.csv'\n",
    "cases.to_csv(output_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Respondent Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.17 Extract Respondent Names from Case Details\n",
    "def extract_respondent(text):\n",
    "    \"\"\"\n",
    "    Extract the respondent's name from case details using regex.\n",
    "    \"\"\"\n",
    "    and_pattern = r'(-?\\s*a\\s*n\\s*d\\s*-?\\s*)'\n",
    "    respondent_pattern = r'(Respondent|Respondents)'\n",
    "    \n",
    "    and_match = re.search(and_pattern, text, re.IGNORECASE)\n",
    "    if and_match:\n",
    "        start = and_match.end()\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "    respondent_match = re.search(respondent_pattern, text[start:], re.IGNORECASE)\n",
    "    if respondent_match:\n",
    "        end = start + respondent_match.start()\n",
    "    else:\n",
    "        end = len(text)\n",
    "    \n",
    "    return text[start:end].strip()\n",
    "\n",
    "# Apply the function to extract respondents\n",
    "cases['respondent'] = cases['caseDetails'].apply(extract_respondent)\n",
    "\n",
    "# Reorder columns to have 'respondent' before 'caseDetails'\n",
    "cases = cases[['caseId', 'respondent', 'caseDetails']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.18 Save Respondent Data to CSV\n",
    "output_path = r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\data\\respondent.csv'\n",
    "cases.to_csv(output_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjudicator Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.19 Extract Adjudicator Names from Case Details\n",
    "def extract_adjudicator(text):\n",
    "    \"\"\"\n",
    "    Extract the adjudicator's name from case details using regex.\n",
    "    \"\"\"\n",
    "    pattern = r'Adjudicator:\\s*(.*?)\\s*Date:'\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return ''\n",
    "\n",
    "# Apply the function to extract adjudicators\n",
    "cases['adjudicator'] = cases['caseDetails'].apply(extract_adjudicator)\n",
    "\n",
    "# Reorder columns to have 'adjudicator' before 'caseDetails'\n",
    "cases = cases[['caseId', 'adjudicator', 'caseDetails']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.20 Save Adjudicator Data to CSV\n",
    "output_path = r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\data\\adjudicator.csv'\n",
    "cases.to_csv(output_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_decision_type(text):\n",
    "    \"\"\"\n",
    "    Extract the decision type between 'Respondent' and 'Adjudicator' from case details.\n",
    "    \"\"\"\n",
    "    pattern = r'Respondent[s]?\\s*(.*?)\\s*Adjudicator[s]?:'\n",
    "    match = re.search(pattern, text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return 'UNKNOWN'\n",
    "\n",
    "# Apply the function to extract decision types\n",
    "cases['decisionType'] = cases['caseDetails'].apply(extract_decision_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.22 Save Decision Type Data to CSV\n",
    "output_path = r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\data\\decision.csv'\n",
    "cases.to_csv(output_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Written Submission and Appearances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.23 Search for Written Submissions and Appearances\n",
    "cases['writtenSubmission'] = cases['caseDetails'].str.contains('written submission|written submissions', case=False, regex=True)\n",
    "cases['appearances'] = cases['caseDetails'].str.contains('APPEARANCES|Appearances', case=False, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.24 Save Written Submission and Appearances Data to CSV\n",
    "output_path = r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\data\\written.csv'\n",
    "cases.to_csv(output_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Protected Grounds & Areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.25 Feature Extraction from Reasoning Column (e.g., Age, Creed, Race)\n",
    "cases = pd.read_csv(r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\data\\reasoning.csv')\n",
    "\n",
    "# Define keywords to extract\n",
    "keywords = ['age', 'ancestry', 'citizenship', 'colour', 'creed', 'disability', 'ethnic origin', 'family status', \n",
    "            'gender identity', 'marital status', 'place of origin', 'public assistance', 'race', 'sexual orientation']\n",
    "\n",
    "for keyword in keywords:\n",
    "    cases[keyword] = cases['reasoning'].str.contains(keyword, case=False, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.26 Save Extracted Features to CSV\n",
    "output_path = r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\data\\protectedQA.csv'\n",
    "cases.to_csv(output_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.27 Text Preprocessing: Removing Punctuation, Numbers, and Lemmatizing\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text by converting to lowercase, removing punctuation, and numbers.\n",
    "    \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "    else:\n",
    "        text = ''\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to 'reasoning' column\n",
    "cases['reasoning'] = cases['reasoning'].apply(preprocess_text)\n",
    "\n",
    "# Lemmatize and remove stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    \"\"\"\n",
    "    Tokenize text and lemmatize words, removing stopwords.\n",
    "    \"\"\"\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "cases['reasoning'] = cases['reasoning'].apply(tokenize_and_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.28 Save Preprocessed Reasoning Data to CSV\n",
    "output_path = r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\data\\reasoningQA.csv'\n",
    "cases.to_csv(output_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Master Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.29 Merge All DataFrames to Create a Master DataFrame\n",
    "# Load other dataframes for merging\n",
    "adjudicator = pd.read_csv(r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\data\\adjudicatorQA.csv')\n",
    "applicant = pd.read_csv(r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\data\\applicantQA.csv')\n",
    "decisionType = pd.read_csv(r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\data\\decisionQA.csv')\n",
    "metadata = pd.read_csv(r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\data\\metadataQA.csv')\n",
    "protectedGrounds = pd.read_csv(r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\data\\protectedQA.csv')\n",
    "respondent = pd.read_csv(r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\data\\respondentQA.csv')\n",
    "representation = pd.read_csv(r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\data\\writtenQA.csv')\n",
    "labels = pd.read_csv(r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\data\\labelsQA.csv')\n",
    "reasoning = pd.read_csv(r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\data\\reasoningQA.csv')\n",
    "\n",
    "# Merge all dataframes on 'caseId'\n",
    "master = labels.merge(applicant, on='caseId', how='inner')\\\n",
    "               .merge(decisionType, on='caseId', how='inner')\\\n",
    "               .merge(metadata, on='caseId', how='inner')\\\n",
    "               .merge(protectedGrounds, on='caseId', how='inner')\\\n",
    "               .merge(respondent, on='caseId', how='inner')\\\n",
    "               .merge(representation, on='caseId', how='inner')\\\n",
    "               .merge(adjudicator, on='caseId', how='inner')\\\n",
    "               .merge(reasoning, on='caseId', how='inner')\n",
    "\n",
    "# Display summary of the master dataframe\n",
    "master.info()\n",
    "master.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.30 Save Master DataFrame to CSV\n",
    "output_path = r'E:\\Vocational\\Lighthouse Labs\\Flex Course\\Projects\\P06_Final Project\\data\\master.csv'\n",
    "master.to_csv(output_path, index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
